# What do Principal Components Actually do Mathematically?

I have recently taken an interest in PCA after watching Professor Gilbert Strangâ€™s [SVD lecture](https://www.youtube.com/watch?v=rYz83XPxiZo). I must have watched at least 15 other videos and 7 different blog posts/papers on PCA since. They are all very excellent resources, but I found myself somewhat unsatisfied. What they do a lot is teaching us the following:
- What the PCA promise is;
- Why that promise is very useful in Data Science;
- How to extract these principal components. (Although I don't agree with how some of them do it by applying SVD on the covariance matrix, but that can be saved for another post.)

Some of them go the extra mile to show us graphically or logically, how the promise is being fulfilled by the principal components:
- Graphically, a transformed vector can be shown to be still clustered with its original group in a plot;
- Logically, a proof can be expressed in mathematical symbols.

My mind was convinced, but my heart was still not. To me, the graphic approach does not provide a visual effect that is striking enough. The logic approach on the other hand, does not even spend enough effort to explain what precisely it's trying to prove. Therefore, the objective of this post is to improve both of these 2 areas - to provide a more striking visual and to establish a more precise goal for our mathematical proof.

Graphically vs numerically Vs mathematical proof

What if we rotate the dataset in a way that all its covariance become 0s?

Invent vs discover


```python

```
